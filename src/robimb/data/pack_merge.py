"""Utilities to merge legacy and production knowledge packs into a single bundle."""
from __future__ import annotations

import argparse
import datetime as dt
import hashlib
import json
import os
from dataclasses import dataclass
from pathlib import Path
from typing import Any, Dict, Iterable, List, Mapping, MutableMapping, Optional, Sequence

from robimb.extraction import resources as extraction_resources

BASE_FILES = {
    "registry": "properties_registry_modular.json",
    "catmap": "category_property_map.json",
    "extractors": extraction_resources.default_path().with_name("extractors_patterns.json"),
    "validators": "validators.json",
    "formulas": "formulas.json",
    "templates": "templates_descriptions.json",
    "views": "views.json",
    "profiles": "profiles.json",
    "contexts": "contexts.json",
    "properties_ext": "properties_registry_extended.json",
}

PROD_FILES = {
    "registry": "prod_properties_registry.json",
    "catmap": "prod_cat_property_map.json",
    "extractors": "prod_extractors_patterns.json",
    "validators": "prod_validators.json",
    "formulas": "prod_formulas.json",
    "templates": "prod_templates_descriptions.json",
    "views": "prod_views.json",
    "profiles": "prod_profiles.json",
    "contexts": "prod_contexts.json",
    "categories": "prod_categories.json",
}


@dataclass
class PackArtifacts:
    """Description of the files generated by :func:`build_merged_pack`."""

    version: str
    generated_at: str
    files: Dict[str, Path]
    manifest_path: Path


def _load_json(path: Path) -> Dict[str, Any]:
    with path.open("r", encoding="utf-8") as handle:
        return json.load(handle)


def _write_json(path: Path, payload: Mapping[str, Any]) -> None:
    path.parent.mkdir(parents=True, exist_ok=True)
    with path.open("w", encoding="utf-8") as handle:
        json.dump(payload, handle, ensure_ascii=False, indent=2)
        handle.write("\n")


def _dedup_list(values: Iterable[Any]) -> List[Any]:
    seen: set[Any] = set()
    result: List[Any] = []
    for item in values:
        marker = json.dumps(item, sort_keys=True, ensure_ascii=False) if isinstance(item, Mapping) else item
        if marker in seen:
            continue
        seen.add(marker)
        result.append(item)
    return result


def _merge_registry(base: Mapping[str, Any], prod: Mapping[str, Any], version: str, generated_at: str) -> Dict[str, Any]:
    base_groups = base.get("groups", {})
    prod_groups = prod.get("groups", {})
    merged_groups: Dict[str, Dict[str, Any]] = {}
    for key in sorted(set(base_groups) | set(prod_groups)):
        base_entry = base_groups.get(key, {})
        prod_entry = prod_groups.get(key, {})
        merged_groups[key] = {
            "label": prod_entry.get("label") or base_entry.get("label"),
            "description": prod_entry.get("description") or base_entry.get("description"),
            "properties": _dedup_list(
                [
                    *prod_entry.get("properties", []),
                    *base_entry.get("properties", []),
                ]
            ),
        }

    base_props = base.get("properties", {})
    prod_props = prod.get("properties", {})
    merged_props: Dict[str, Dict[str, Any]] = {}
    for key in sorted(set(base_props) | set(prod_props)):
        merged: Dict[str, Any] = {}
        for source in (base_props.get(key, {}), prod_props.get(key, {})):
            for field, value in source.items():
                if value is None:
                    continue
                if isinstance(value, list):
                    merged[field] = _dedup_list([*merged.get(field, []), *value])
                elif isinstance(value, dict):
                    merged[field] = {**merged.get(field, {}), **value}
                else:
                    merged[field] = value
        for field in ("id", "group", "label", "type"):
            merged.setdefault(field, prod_props.get(key, {}).get(field) or base_props.get(key, {}).get(field))
        merged_props[key] = merged

    base_enums = base.get("enums", {})
    prod_enums = prod.get("enums", {})
    merged_enums: Dict[str, Dict[str, Any]] = {}
    for key in sorted(set(base_enums) | set(prod_enums)):
        values = [
            *prod_enums.get(key, {}).get("values", []),
            *base_enums.get(key, {}).get("values", []),
        ]
        merged_enums[key] = {"values": _dedup_list(values)}

    registry: Dict[str, Any] = {
        "version": version,
        "generated_at": generated_at,
        "units": prod.get("units") or base.get("units"),
        "groups": merged_groups,
        "properties": merged_props,
        "enums": merged_enums,
    }
    if registry.get("units") is None:
        registry.pop("units")
    return registry


def _merge_extractors(base: Mapping[str, Any], prod: Mapping[str, Any], version: str, generated_at: str) -> Dict[str, Any]:
    patterns: Dict[str, Dict[str, Any]] = {}

    def add_pattern(entry: Mapping[str, Any]) -> None:
        prop_id = entry["property_id"]
        merged = patterns.setdefault(prop_id, {"property_id": prop_id, "regex": [], "normalizers": []})
        merged["regex"] = _dedup_list([*merged.get("regex", []), *entry.get("regex", [])])
        merged["normalizers"] = _dedup_list([*merged.get("normalizers", []), *entry.get("normalizers", [])])
        for field, value in entry.items():
            if field in {"property_id", "regex", "normalizers"}:
                continue
            merged[field] = value

    for item in prod.get("patterns", []):
        add_pattern(item)
    for item in base.get("patterns", []):
        add_pattern(item)

    normalizers: Dict[str, Any] = {}
    normalizers.update(base.get("normalizers", {}))
    for key, value in prod.get("normalizers", {}).items():
        if key not in normalizers:
            normalizers[key] = value
        else:
            existing = normalizers[key]
            if isinstance(existing, list) and isinstance(value, list):
                normalizers[key] = _dedup_list([*existing, *value])
            elif isinstance(existing, dict) and isinstance(value, dict):
                merged_dict = dict(existing)
                merged_dict.update(value)
                normalizers[key] = merged_dict
            else:
                normalizers[key] = value

    return {
        "version": version,
        "generated_at": generated_at,
        "patterns": sorted(patterns.values(), key=lambda item: item["property_id"]),
        "normalizers": normalizers,
    }


def _merge_validators(base: Mapping[str, Any], prod: Mapping[str, Any], version: str, generated_at: str) -> Dict[str, Any]:
    rules = _dedup_list([*prod.get("rules", []), *base.get("rules", [])])
    return {"version": version, "generated_at": generated_at, "rules": rules}


def _merge_formulas(base: Mapping[str, Any], prod: Mapping[str, Any], version: str, generated_at: str) -> Dict[str, Any]:
    by_target: Dict[str, Dict[str, Any]] = {}

    def add(entry: Mapping[str, Any]) -> None:
        target = entry["target"]
        merged = by_target.setdefault(target, {"target": target})
        for field, value in entry.items():
            if field == "requires" and value:
                merged[field] = _dedup_list([*merged.get(field, []), *value])
            elif field != "target":
                merged[field] = value or merged.get(field)

    for item in prod.get("computed", []):
        add(item)
    for item in base.get("computed", []):
        add(item)

    return {
        "version": version,
        "generated_at": generated_at,
        "computed": sorted(by_target.values(), key=lambda item: item["target"]),
    }


def _merge_views(base: Mapping[str, Any], prod: Mapping[str, Any], version: str, generated_at: str) -> Dict[str, Any]:
    by_id: Dict[str, Dict[str, Any]] = {}

    def add(entry: Mapping[str, Any]) -> None:
        view_id = entry["id"]
        merged = by_id.setdefault(view_id, {"id": view_id, "properties": []})
        merged["label"] = entry.get("label") or merged.get("label")
        merged["properties"] = _dedup_list([*merged.get("properties", []), *entry.get("properties", [])])

    for item in prod.get("views", []):
        add(item)
    for item in base.get("views", []):
        add(item)

    return {
        "version": version,
        "generated_at": generated_at,
        "views": sorted(by_id.values(), key=lambda item: item["id"]),
    }


def _merge_templates(base: Mapping[str, Any], prod: Mapping[str, Any], version: str, generated_at: str) -> Dict[str, Any]:
    by_cat: Dict[str, Dict[str, Any]] = {}

    def add(entry: Mapping[str, Any]) -> None:
        category = entry["category"]
        merged = by_cat.setdefault(category, {"category": category})
        merged["template"] = entry.get("template") or merged.get("template")
        merged["placeholders_required"] = _dedup_list(
            [*merged.get("placeholders_required", []), *entry.get("placeholders_required", [])]
        )
        merged["fallbacks"] = _dedup_list([*merged.get("fallbacks", []), *entry.get("fallbacks", [])])

    for item in prod.get("templates", []):
        add(item)
    for item in base.get("templates", []):
        add(item)

    return {
        "version": version,
        "generated_at": generated_at,
        "templates": sorted(by_cat.values(), key=lambda item: item["category"]),
    }


def _profile_key(profile: Mapping[str, Any]) -> str:
    profile_id = profile.get("id")
    if profile_id:
        return profile_id.split(".", 1)[-1]
    return f"{profile.get('discipline')}::{profile.get('phase')}"


def _merge_profiles(base: Mapping[str, Any], prod: Mapping[str, Any], version: str, generated_at: str) -> Dict[str, Any]:
    merged: Dict[str, Dict[str, Any]] = {}

    def add(entry: Mapping[str, Any], prefer_prod: bool) -> None:
        key = _profile_key(entry)
        target = merged.setdefault(key, {"id": key})
        if prefer_prod and entry.get("id"):
            target["id"] = entry["id"]
        elif target.get("id", "").startswith("profile."):
            target["id"] = target["id"].split(".", 1)[-1]

        for field in ("label", "discipline", "phase"):
            value = entry.get(field)
            if value and (prefer_prod or field not in target or target[field] is None):
                target[field] = value
            elif field not in target:
                target[field] = value

        for field in ("required_groups_add", "recommended_groups_add"):
            target[field] = _dedup_list([*target.get(field, []), *entry.get(field, [])])

        if entry.get("property_overrides"):
            overrides: MutableMapping[str, Any] = target.setdefault("property_overrides", {})
            for prop_id, override in entry["property_overrides"].items():
                current = overrides.get(prop_id, {})
                combined = dict(current)
                combined.update(override)
                overrides[prop_id] = combined

    for item in prod.get("profiles", []):
        add(item, prefer_prod=True)
    for item in base.get("profiles", []):
        adjusted = dict(item)
        if adjusted.get("id"):
            adjusted["id"] = adjusted["id"].split(".", 1)[-1]
        add(adjusted, prefer_prod=False)

    profiles_list: List[Dict[str, Any]] = []
    for profile in merged.values():
        if not profile.get("property_overrides"):
            profile.pop("property_overrides", None)
        profiles_list.append(profile)

    profiles_list.sort(key=lambda item: item["id"])
    return {
        "version": version,
        "generated_at": generated_at,
        "profiles": profiles_list,
    }


def _merge_contexts(base: Mapping[str, Any], prod: Mapping[str, Any], version: str, generated_at: str) -> Dict[str, Any]:
    base_dims = base.get("dimensions", {})
    prod_dims = prod.get("dimensions", {})
    merged_dims: Dict[str, Dict[str, Any]] = {}
    for key in sorted(set(base_dims) | set(prod_dims)):
        base_entry = base_dims.get(key, {})
        prod_entry = prod_dims.get(key, {})
        merged_dims[key] = {
            "label": base_entry.get("label") or prod_entry.get("label"),
            "values": _dedup_list([*prod_entry.get("values", []), *base_entry.get("values", [])]),
        }
    return {
        "version": version,
        "generated_at": generated_at,
        "dimensions": merged_dims,
    }


def _merge_catmap(
    prod: Mapping[str, Any],
    base: Mapping[str, Any],
    extended: Mapping[str, Any],
    version: str,
    generated_at: str,
) -> Dict[str, Any]:
    defaults = base.get("default_requirements_by_super", {})
    overrides = base.get("category_overrides", {})
    ifc_defaults = base.get("ifc_defaults_by_super", {})

    ext_lookup: Dict[tuple[str, str], Mapping[str, Any]] = {}
    for key, value in extended.items():
        if key == "_schema" or "|" not in key:
            continue
        super_label, cat_label = [part.strip() for part in key.split("|", 1)]
        ext_lookup[(super_label, cat_label)] = value

    merged_entries: List[Dict[str, Any]] = []
    for entry in prod.get("mappings", []):
        merged = dict(entry)
        super_label = entry["super_label"]
        cat_label = entry["cat_label"]

        def extend(field: str, values: Optional[Sequence[Any]]) -> None:
            if not values:
                return
            merged[field] = _dedup_list([*merged.get(field, []), *values])

        extend("groups_required", defaults.get(super_label, {}).get("required_groups"))
        extend("groups_recommended", defaults.get(super_label, {}).get("recommended_groups"))

        override = overrides.get(cat_label, {})
        extend("props_required", override.get("required"))
        extend("props_recommended", override.get("recommended"))
        extend("groups_required", override.get("required_groups"))
        extend("groups_recommended", override.get("recommended_groups"))
        extend("ifc_classes", override.get("ifc_classes"))
        if override.get("rules"):
            merged["rules"] = _dedup_list([*merged.get("rules", []), *override["rules"]])

        extend("ifc_classes", ifc_defaults.get(super_label, {}).get("ifc_classes"))

        ext_info = ext_lookup.get((super_label, cat_label))
        if ext_info:
            if ext_info.get("priority"):
                merged["slot_priority"] = list(ext_info.get("priority", []))
            if ext_info.get("slots"):
                merged["slots"] = ext_info.get("slots")

        merged_entries.append(merged)

    merged_entries.sort(key=lambda item: (item["super_label"], item["cat_label"]))
    return {
        "version": version,
        "generated_at": generated_at,
        "mappings": merged_entries,
    }


def _merge_categories(prod: Mapping[str, Any], version: str, generated_at: str) -> Dict[str, Any]:
    merged = {
        "version": version,
        "generated_at": generated_at,
        "super": sorted(prod.get("super", []), key=lambda item: item["id"]),
        "cat": sorted(prod.get("cat", []), key=lambda item: item["id"]),
    }
    return merged


def _compute_manifest(files: Mapping[str, Path], generated_at: str, *, manifest_dir: Optional[Path] = None) -> Dict[str, Any]:
    entries: List[Dict[str, Any]] = []
    manifest_dir = manifest_dir or Path.cwd()
    for key, path in sorted(files.items()):
        digest = hashlib.sha256(path.read_bytes()).hexdigest()
        rel_path = os.path.relpath(path, manifest_dir)
        entries.append(
            {
                "name": key,
                "path": rel_path,
                "sha256": digest,
                "size": path.stat().st_size,
            }
        )
    return {"generated_at": generated_at, "files": entries}


def build_merged_pack(
    data_dir: Path,
    output_dir: Path,
    version: str = "1.1.0",
    timestamp: Optional[str] = None,
) -> PackArtifacts:
    """Merge base and production knowledge packs into ``output_dir``.

    Parameters
    ----------
    data_dir:
        Directory that contains the source JSON files (both legacy and production ones).
    output_dir:
        Target directory that will receive the merged knowledge pack files.
    version:
        Semantic version assigned to the generated files.
    timestamp:
        Optional ISO timestamp. When missing the current UTC time is used.
    """

    data_dir = Path(data_dir)
    output_dir = Path(output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)

    base_data = {}
    for key, location in BASE_FILES.items():
        candidate = Path(location)
        source = candidate if candidate.is_absolute() else data_dir / candidate
        base_data[key] = _load_json(source)

    prod_data = {key: _load_json(data_dir / Path(path)) for key, path in PROD_FILES.items()}

    generated_at = timestamp or dt.datetime.utcnow().replace(microsecond=0).isoformat() + "Z"

    registry = _merge_registry(base_data["registry"], prod_data["registry"], version, generated_at)
    extractors = _merge_extractors(base_data["extractors"], prod_data["extractors"], version, generated_at)
    validators = _merge_validators(base_data["validators"], prod_data["validators"], version, generated_at)
    formulas = _merge_formulas(base_data["formulas"], prod_data["formulas"], version, generated_at)
    views = _merge_views(base_data["views"], prod_data["views"], version, generated_at)
    templates = _merge_templates(base_data["templates"], prod_data["templates"], version, generated_at)
    profiles = _merge_profiles(base_data["profiles"], prod_data["profiles"], version, generated_at)
    contexts = _merge_contexts(base_data["contexts"], prod_data["contexts"], version, generated_at)
    categories = _merge_categories(prod_data["categories"], version, generated_at)
    catmap = _merge_catmap(prod_data["catmap"], base_data["catmap"], base_data["properties_ext"], version, generated_at)

    resource_extractors_path = extraction_resources.default_path()

    files: Dict[str, Path] = {
        "registry": output_dir / "registry.json",
        "validators": output_dir / "validators.json",
        "formulas": output_dir / "formulas.json",
        "views": output_dir / "views.json",
        "templates": output_dir / "templates.json",
        "profiles": output_dir / "profiles.json",
        "contexts": output_dir / "contexts.json",
        "categories": output_dir / "categories.json",
        "catmap": output_dir / "catmap.json",
    }

    files["extractors"] = resource_extractors_path

    payloads = {
        "registry": registry,
        "extractors": extractors,
        "validators": validators,
        "formulas": formulas,
        "views": views,
        "templates": templates,
        "profiles": profiles,
        "contexts": contexts,
        "categories": categories,
        "catmap": catmap,
    }

    for key, path in files.items():
        _write_json(path, payloads[key])

    manifest = _compute_manifest(files, generated_at, manifest_dir=output_dir)
    manifest_path = output_dir / "manifest.json"
    _write_json(manifest_path, manifest)
    files["manifest"] = manifest_path

    return PackArtifacts(version=version, generated_at=generated_at, files=files, manifest_path=manifest_path)


def write_pack_index(artifacts: PackArtifacts, current_dir: Path) -> Path:
    """Create/overwrite ``pack.json`` pointing to the generated files."""

    current_dir = Path(current_dir)
    current_dir.mkdir(parents=True, exist_ok=True)
    index_path = current_dir / "pack.json"
    rel_files = {
        key: os.path.relpath(Path(path).resolve(), current_dir.resolve())
        for key, path in artifacts.files.items()
    }
    payload = {
        "version": artifacts.version,
        "generated_at": artifacts.generated_at,
        "files": rel_files,
    }
    _write_json(index_path, payload)
    return index_path


def _parse_args(argv: Optional[Sequence[str]] = None) -> argparse.Namespace:
    parser = argparse.ArgumentParser(description="Merge legacy and production knowledge packs into a single bundle.")
    parser.add_argument("--data-dir", type=Path, default=Path("data"), help="Directory containing the source JSON files")
    parser.add_argument("--out-dir", type=Path, default=Path("pack") / "v1", help="Output directory for the merged pack")
    parser.add_argument("--version", type=str, default="1.1.0", help="Version assigned to the merged pack")
    parser.add_argument(
        "--current-dir",
        type=Path,
        default=Path("pack") / "current",
        help="Directory where the generated pack.json will be placed",
    )
    parser.add_argument(
        "--no-current",
        action="store_true",
        help="Skip writing pack/current/pack.json",
    )
    return parser.parse_args(argv)


def main(argv: Optional[Sequence[str]] = None) -> PackArtifacts:
    args = _parse_args(argv)
    artifacts = build_merged_pack(args.data_dir, args.out_dir, version=args.version)
    if not args.no_current:
        write_pack_index(artifacts, args.current_dir)
    return artifacts


if __name__ == "__main__":  # pragma: no cover - manual execution helper
    main()
