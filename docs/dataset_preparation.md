# Pipeline di preparazione dei dataset

Il comando `robimb convert` delega la logica di preprocessing al modulo `utils/data_utils.py`, che fornisce funzioni per leggere i JSONL grezzi, applicare la mappatura delle etichette e arricchire i record con proprietà estratte automaticamente. La funzione `load_jsonl_to_df` converte i file `train_classif.jsonl` e `val_classif.jsonl` in `DataFrame` Pandas pronti per l'elaborazione.

La funzione `prepare_classification_dataset` è responsabile della maggior parte del lavoro: crea o carica le label map (`create_or_load_label_maps`), risolve gli indici numerici `super_label` e `cat_label`, filtra i record non mappabili e associa a ciascun elemento la struttura di proprietà definita nel registry passato via CLI (se fornito tramite `--properties-registry`). Se disponibile, un pacchetto di estrattori – di default il file `pack/current/extractors.json` risolto da `robimb.extraction.resources.load_default()` – viene passato a `features.extractors.extract_properties` per popolare automaticamente i valori di proprietà partendo dal testo grezzo. Il percorso può essere personalizzato via CLI (`--extractors-pack`) indicando un JSON alternativo oppure un altro bundle versionato con la stessa struttura (`pack/v*/extractors.json`).

Durante la preparazione vengono considerate esclusioni opzionali tramite `done_uids.txt`, viene calcolato lo split train/val deterministico e vengono restituiti `DataFrame` arricchiti con le colonne `property_schema` e `properties`. Questi dataset vengono poi serializzati con `save_datasets`, che scrive JSONL preprocessati (`train_processed.jsonl`, `val_processed.jsonl`) e i report di supporto. La stessa pipeline può generare un corpus per TAPT tramite `prepare_mlm_corpus`, concatenando le descrizioni testuali e le label description quando richiesto.
