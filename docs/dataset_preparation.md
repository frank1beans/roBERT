# Pipeline di preparazione dei dataset

Il comando `robimb convert` delega la logica di preprocessing al modulo `utils/dataset_prep.py`, che coordina la lettura dei JSONL grezzi, l'applicazione delle label map e l'arricchimento dei record con le proprietà estratte automaticamente. Per il caricamento rapido dei file è disponibile `utils/sampling.load_jsonl_to_df`, che converte `train_classif.jsonl` e `val_classif.jsonl` in `DataFrame` Pandas pronti per l'elaborazione.

La funzione `prepare_classification_dataset` è responsabile della maggior parte del lavoro: crea o carica le label map (`create_or_load_label_maps`), risolve gli indici numerici `super_label` e `cat_label`, filtra i record non mappabili e associa a ciascun elemento la struttura di proprietà definita nel registry passato via CLI (se fornito tramite `--properties-registry`). Se disponibile, un pacchetto di estrattori – di default il file `pack/current/extractors.json` – viene passato a `robimb.extraction.legacy.extract_properties` per popolare automaticamente i valori di proprietà partendo dal testo grezzo. Il percorso può essere personalizzato via CLI (`--extractors-pack`) indicando un JSON alternativo oppure un altro bundle versionato con la stessa struttura (`pack/v*/extractors.json`).

Durante la preparazione vengono considerate esclusioni opzionali tramite `done_uids.txt`, viene calcolato lo split train/val deterministico e vengono restituiti `DataFrame` arricchiti con le colonne `property_schema` e `properties`. Questi dataset vengono poi serializzati con `save_datasets`, che scrive JSONL preprocessati (`train_processed.jsonl`, `val_processed.jsonl`) e i report di supporto. La stessa pipeline può generare un corpus per TAPT tramite `prepare_mlm_corpus`, concatenando le descrizioni testuali e le label description quando richiesto.
